{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Glow_mnist.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MokkeMeguru/glow-realnvp-tutorial/blob/master/examples/jupyter_notebooks/Glow_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZrwVQsM9TiUw"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giivatOA26nF",
        "colab_type": "text"
      },
      "source": [
        "##### Original code\n",
        "https://github.com/MokkeMeguru/glow-realnvp-tutorial/\n",
        "\n",
        "If you have any Problem, please let me(@MokkeMeguru) it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ecn48QS26nG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ltPJCG6pAUoc"
      },
      "source": [
        "# TFP Bijector: Glow x MNIST\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/glow_mnist.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/glow_mnist.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WRVR-tGTR31S"
      },
      "source": [
        "In this example we show how to build a Glow using TFP's \"Bijector.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXZLuDpH26nM",
        "colab_type": "text"
      },
      "source": [
        "# Glow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DknOAPpf26nN",
        "colab_type": "text"
      },
      "source": [
        "Difference from RealNVP \n",
        "\n",
        "- Invertible 1x1 convolution\n",
        "- Actnorm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZOHWrYJ26nO",
        "colab_type": "text"
      },
      "source": [
        "## Invertible 1x1 convolution\n",
        "\n",
        "RealNVP uses Permutation by the channel dimention for Affine Coupling Layer. (Intuitively„ÄÅwe want to replace $x_a$ and $x_b$, I think...)\n",
        "\n",
        "In Glow, the Permutaion is changed to the Invertible 1x1 convolution he proposed. It's a learnable layer (bijector).\n",
        "\n",
        "formula\n",
        "\n",
        "1. forward function\n",
        "2. reverse function\n",
        "3. log determinant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mOcvF9W26nP",
        "colab_type": "text"
      },
      "source": [
        "## Actnorm\n",
        "Instead of Batch Normalization\n",
        "\n",
        "Week points of Batch Normalization\n",
        "1. IF batch size is small, it may decrease performance. (In flow base model, Batch size is often too small since this model needs large memory...)\n",
        "\n",
        "\n",
        "Behavior of Actnorm\n",
        "1. normalize by **channel-wise** , using the **first batch**\n",
        "2. initialize weights by the normalize parameter. (bias and scale)\n",
        "3. learn these weights since next batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzXR51yd26nQ",
        "colab_type": "text"
      },
      "source": [
        "# Problem Setting\n",
        "Multivariate normal distribution $\\leftrightarrow$ MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V69lrhH26nR",
        "colab_type": "text"
      },
      "source": [
        "# Use Tensorflow\n",
        "## Implementation Plan\n",
        "0. Preprocess\n",
        "1. Create Dataset\n",
        "2. Build Multi-Scale Model\n",
        "    1. Invertible 1x1 convolution Bijector\n",
        "    2. Actnorm Bijector\n",
        "    3. Multi-Scale Model\n",
        "4. Build Model\n",
        "    1. TransformDistribution\n",
        "    2. Loss, Optimizar\n",
        "    3. Training\n",
        "    4. Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv4nyUPO26nS",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bu8x3TB26nT",
        "colab_type": "code",
        "outputId": "92baad80-d85d-4666-8d87-c0fe326a7473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.1.0rc0 tensorflow_probability==0.8.0 \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "\n",
        "print('tensorflow: ', tf.__version__)\n",
        "print('tensorflow-probability: ', tfp.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (2.1.0rc0)\n",
            "Requirement already satisfied: tensorflow_probability==0.8.0 in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0rc0) (2.0.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0rc0) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0rc0) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0rc0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0rc0) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0rc0) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0rc0) (1.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0rc0) (0.1.8)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0rc0) (0.33.6)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0rc0) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0rc0) (3.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0rc0) (1.0.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0rc0) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0rc0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0rc0) (1.17.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0rc0) (1.15.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow_probability==0.8.0) (4.4.1)\n",
            "Requirement already satisfied: cloudpickle==1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_probability==0.8.0) (1.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0rc0) (42.0.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0rc0) (0.16.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0rc0) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0rc0) (2.21.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0rc0) (1.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0rc0) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.1.0rc0) (2.8.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0rc0) (1.3.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0rc0) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0rc0) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0rc0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0rc0) (2019.11.28)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0rc0) (0.2.7)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0rc0) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0rc0) (3.1.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0rc0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0rc0) (0.4.8)\n",
            "tensorflow:  2.1.0-rc0\n",
            "tensorflow-probability:  0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcraszzM26nX",
        "colab_type": "text"
      },
      "source": [
        "## Create Dataset\n",
        "ref. [realnvp's tutorial](./RealNVP_mnist_en.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJOjNM4j26nY",
        "colab_type": "text"
      },
      "source": [
        "### Create Target Distibution ($z$)\n",
        "In this part, use Multivariate normal distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4nkknv926nZ",
        "colab_type": "code",
        "outputId": "8e3e5789-6ce3-435b-c947-7e8b75a641a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Tensorflow's distribution\n",
        "z = tf.random.normal([2, 2, 256])\n",
        "print('Tensorflow             : shape: {} mean: {:.6f} sd: {:.6f}'.format(z.shape, tf.math.reduce_mean(z), tf.math.reduce_std(z)))\n",
        "# Tensorflow Probability's distribution\n",
        "target_dist = tfd.Normal(loc=0., scale=1.)\n",
        "z = target_dist.sample([2, 2, 256])\n",
        "print('Tensorflow Probability : shape: {} mean: {:.6f} sd: {:.6f}'.format(z.shape, tf.math.reduce_mean(z), tf.math.reduce_std(z)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow             : shape: (2, 2, 256) mean: -0.006551 sd: 1.040703\n",
            "Tensorflow Probability : shape: (2, 2, 256) mean: 0.060990 sd: 1.012849\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcXOFr4l26nb",
        "colab_type": "text"
      },
      "source": [
        "### Create Original Distribution (MNIST)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBWMf4p026nc",
        "colab_type": "code",
        "outputId": "5a60fda2-5e36-4e9d-b848-fb77a09fc274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "print('train_dataset: {} images'.format(len(train_x)))\n",
        "print('test_dataset : {} images'.format(len(test_x)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_dataset: 60000 images\n",
            "test_dataset : 10000 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-4YT5d426ne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def _parse_function(img, label):\n",
        "    feature = {}\n",
        "    img = tf.pad(img, paddings=[[2, 2], [2, 2]], mode=\"CONSTANT\")\n",
        "    img = tf.expand_dims(img, axis=-1)\n",
        "    img = tf.reshape(img, [32, 32, 1])\n",
        "    img = tf.cast(img, dtype=tf.float32)\n",
        "    img = (img / (255.0 / 2)) - 1\n",
        "    feature[\"img\"] = img\n",
        "    feature[\"label\"] = label\n",
        "    return feature\n",
        "\n",
        "\n",
        "train_dataset_raw = tf.data.Dataset.from_tensor_slices((train_x, train_y)).map(\n",
        "    _parse_function\n",
        ")\n",
        "test_dataset_raw = tf.data.Dataset.from_tensor_slices((test_x, test_y)).map(\n",
        "    _parse_function\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FEhcOYO26ng",
        "colab_type": "code",
        "outputId": "bd4af67b-e4b9-484a-9832-b1d17b0688ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "for image_features in train_dataset_raw.take(1):\n",
        "    plt.imshow(tf.squeeze(image_features['img'], axis=-1), cmap='gray_r')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndpEgFHaRIP4fM1P6z1+fNGAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_umVJcI626ni",
        "colab_type": "text"
      },
      "source": [
        "## Build Multi-Scale Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tHFaVoI26ni",
        "colab_type": "text"
      },
      "source": [
        "### Invertive 1x1 convolution Bijector\n",
        "ref. \n",
        "- Tensorflow Probability's implementation tensorflow_probability/bijector/MatvecLU \n",
        "- the issue https://github.com/tensorflow/probability/issues/545 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "id": "c7ZJFqln26nj",
        "colab_type": "code",
        "outputId": "463a4dae-99f7-4699-996b-78e71d505647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "def trainable_lu_factorization(event_size,\n",
        "                               batch_shape=(),\n",
        "                               seed=None,\n",
        "                               dtype=tf.float32,\n",
        "                               name=None):\n",
        "    with tf.name_scope('trainable_lu_factorization'):\n",
        "        event_size = tf.convert_to_tensor(event_size,\n",
        "                                          dtype=tf.int32,\n",
        "                                          name='event_size')\n",
        "        batch_shape = tf.convert_to_tensor(batch_shape,\n",
        "                                           dtype=event_size.dtype,\n",
        "                                           name='batch_shape')\n",
        "        random_matrix = tf.Variable(tf.random.uniform(\n",
        "            shape=tf.concat([batch_shape, [event_size, event_size]], axis=0),\n",
        "            dtype=dtype,\n",
        "            seed=seed,\n",
        "        ),\n",
        "                                    name='conv1x1_weights')\n",
        "\n",
        "        def lu_p(m):\n",
        "            return tf.linalg.lu(tf.linalg.qr(m).q)\n",
        "\n",
        "        # lower_upper = tfp.util.DeferredTensor(lambda m: lu_p(m)[0],\n",
        "        #                                       random_matrix)\n",
        "        # permutation = tfp.util.DeferredTensor(lambda m: lu_p(m)[1],\n",
        "        #                                       random_matrix,\n",
        "        #                                       # trainable=False,\n",
        "        #                                       dtype=tf.int32,\n",
        "        #                                       shape=random_matrix.shape[:-1])\n",
        "        lower_upper = tf.Variable(lu_p(random_matrix)[0], name='lower_upper')\n",
        "        # ref https://github.com/tensorflow/probability/issues/545\n",
        "        permutation = tf.Variable(lu_p(random_matrix)[1], trainable=False, name='permutation')\n",
        "        return lower_upper, permutation\n",
        "\n",
        "\n",
        "def build_model(channels=3):\n",
        "    # conv1x1 setup\n",
        "    t_lower_upper, t_permutation = trainable_lu_factorization(channels)\n",
        "    conv1x1 = tfb.MatvecLU(t_lower_upper, t_permutation, name='MatvecLU')\n",
        "    print('conv1x1 variable\\n', conv1x1.variables)\n",
        "    inv_conv1x1 = tfb.Invert(conv1x1)\n",
        "\n",
        "    # forward setup\n",
        "    fwd = tfp.layers.DistributionLambda(\n",
        "        lambda x: conv1x1(tfd.Deterministic(x)))\n",
        "    fwd.vars = conv1x1.trainable_variables\n",
        "\n",
        "    # inverse setup\n",
        "    inv = tfp.layers.DistributionLambda(\n",
        "        lambda x: inv_conv1x1(tfd.Deterministic(x)))\n",
        "    inv.vars = inv_conv1x1.trainable_variables\n",
        "    \n",
        "    x: tf.Tensor = tf.keras.Input(shape=[28, 28, channels])\n",
        "    fwd_x: tfp.distributions.TransformedDistribution = fwd(x)\n",
        "    rev_fwd_x: tfp.distributions.TransformedDistribution = inv(fwd_x)\n",
        "    example_model = tf.keras.Model(inputs=x, outputs=rev_fwd_x, name='conv1x1')\n",
        "    return example_model\n",
        "\n",
        "\n",
        "def test_conv1x1():\n",
        "    example_model = build_model()\n",
        "    example_model.trainable = True\n",
        "    example_model.summary()\n",
        "\n",
        "    real_x = tf.random.uniform(shape=[2, 28, 28, 3], dtype=tf.float32)\n",
        "    if example_model.weights == []:\n",
        "        print('No Trainable Variable exists')\n",
        "    else:\n",
        "        print('Some Trainable Variables exist')\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(real_x)\n",
        "        out_x = example_model(real_x)\n",
        "        out_x = out_x\n",
        "        loss = out_x - real_x\n",
        "    print(tf.math.reduce_sum(real_x - out_x))\n",
        "    # => nealy 0\n",
        "    # ex. tf.Tensor(1.3522818e-05, shape=(), dtype=float32)\n",
        "\n",
        "    try:\n",
        "        print(tape.gradient(loss, real_x).shape)\n",
        "    except Exception as e:\n",
        "        print('Cannot Calculate Gradient')\n",
        "        print(e)\n",
        "        \n",
        "test_conv1x1()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1x1 variable\n",
            " (<tf.Variable 'trainable_lu_factorization/lower_upper:0' shape=(3, 3) dtype=float32, numpy=\n",
            "array([[-0.8432872 ,  0.5080173 ,  0.17545667],\n",
            "       [ 0.6176807 , -1.0058267 , -0.6081466 ],\n",
            "       [ 0.15708762,  0.5892123 ,  1.1789662 ]], dtype=float32)>, <tf.Variable 'trainable_lu_factorization/permutation:0' shape=(3,) dtype=int32, numpy=array([0, 1, 2], dtype=int32)>)\n",
            "Model: \"conv1x1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 28, 28, 3)]       0         \n",
            "_________________________________________________________________\n",
            "distribution_lambda (Distrib ((None, 28, 28, 3), (None 9         \n",
            "_________________________________________________________________\n",
            "distribution_lambda_1 (Distr ((None, 28, 28, 3), (None 9         \n",
            "=================================================================\n",
            "Total params: 9\n",
            "Trainable params: 9\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Some Trainable Variables exist\n",
            "tf.Tensor(-1.732874e-05, shape=(), dtype=float32)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_grad.py:563: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.identity instead.\n",
            "(2, 28, 28, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD1WYX1p26nn",
        "colab_type": "text"
      },
      "source": [
        "### Actnorm Bijector\n",
        "Notice: Initialize by the **First Batch**\n",
        "\n",
        "‚Äª By the way, Actnorm implementation by OpenAI https://github.com/openai/glow/blob/master/tfops.py#L71-L87„ÄÄis not same as their paper's formula, I think...     \n",
        "Forward's formula may be $(x + b) \\odot s$ ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DojZGKt26no",
        "colab_type": "code",
        "outputId": "517bd8dc-5dfd-464c-e414-960643458f97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "class Actnorm(tfb.Bijector):\n",
        "    def __init__(self, channels:int, validate_args=False, name='actnorm', log_scale_factor=1.0):\n",
        "        super(Actnorm, self).__init__(\n",
        "            # this bijector affect vector-wise (channel-wise) => forward_min_event_ndims=1\n",
        "            forward_min_event_ndims=1,\n",
        "            validate_args=validate_args,\n",
        "            name=name\n",
        "        )\n",
        "        self.log_scale_factor = log_scale_factor\n",
        "        self.initialized = False\n",
        "        self.log_scale = tf.Variable(tf.random.normal([channels]))\n",
        "        self.bias = tf.Variable(tf.random.normal([channels]))\n",
        "        \n",
        "    def setStat(self, x):\n",
        "        mean = tf.math.reduce_mean(x, axis=[0, 1, 2])\n",
        "        var = tf.math.reduce_mean((x - mean) ** 2, axis=[0, 1, 2])\n",
        "        stdvar = tf.math.sqrt(var) + 1e-6\n",
        "        log_scale = tf.math.log(1./ stdvar / self.log_scale_factor) * self.log_scale_factor\n",
        "        self.bias.assign(- mean)\n",
        "        self.log_scale.assign(log_scale)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        if not self.initialized:\n",
        "            self.setStat(x)\n",
        "            self.initialized = True\n",
        "        return (x + self.bias) * tf.exp(self.log_scale)\n",
        "    \n",
        "    def _inverse(self, y):\n",
        "        if not self.initialized:\n",
        "            self.setStat(y)\n",
        "            self.initialized = True\n",
        "        return y * tf.exp(- self.log_scale) - self.bias\n",
        "    \n",
        "    def _forward_log_det_jacobian(self, x):\n",
        "        return tf.reduce_sum(self.log_scale)\n",
        "\n",
        "    def _inverse_log_det_jacobian(self, y):\n",
        "        return - tf.reduce_sum(self.log_scale)\n",
        "    \n",
        "def test_actnorm():\n",
        "    actnorm = Actnorm(4)\n",
        "    x = tf.random.normal([100, 16, 16, 4]) + 100\n",
        "    y = actnorm.forward(x)\n",
        "    z = actnorm.inverse(y)\n",
        "    print('input: x', tf.reduce_mean(x, axis=[0, 1, 2]).numpy())\n",
        "    print('output: y', tf.reduce_mean(y, axis=[0, 1, 2]).numpy())\n",
        "    print('inverse: z', tf.reduce_mean(z, axis=[0, 1, 2]).numpy())\n",
        "    print('log_det_jacobian: ', actnorm.forward_log_det_jacobian(y, event_ndims=3).numpy())\n",
        "\n",
        "test_actnorm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input: x [100.00512   99.998825  99.99571   99.99652 ]\n",
            "output: y [-7.5314938e-06  5.6929143e-06 -7.4279310e-06  4.2149422e-06]\n",
            "inverse: z [100.00512   99.998825  99.99571   99.99652 ]\n",
            "log_det_jacobian:  0.6981194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr2b3QT526nq",
        "colab_type": "text"
      },
      "source": [
        "### Multi-Scale Model\n",
        "Implement blueprint    \n",
        "![](https://github.com/MokkeMeguru/glow-realnvp-tutorial/blob/master/examples/img/multi-scale-arch.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNNqQQjI26nq",
        "colab_type": "text"
      },
      "source": [
        "#### Some Utility Class (see. RealNVP_mnist.py)\n",
        "ref. https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/models/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "Jlkvcob226nr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Layer, Conv2D\n",
        "\n",
        "# ref RealNVP_mnist.ipynb\n",
        "class NN(Layer):\n",
        "    def __init__(self,\n",
        "                 input_shape,\n",
        "                 n_hidden=[512, 512],\n",
        "                 kernel_size=[[3, 3], [1, 1]],\n",
        "                 strides=[[1, 1], [1, 1]],\n",
        "                 activation=\"relu\",\n",
        "                 name=None):\n",
        "        if name:\n",
        "            super(NN, self).__init__(name=name)\n",
        "        else:\n",
        "            super(NN, self).__init__()\n",
        "        layer_list = []\n",
        "        for i, (hidden, kernel,\n",
        "                stride) in enumerate(zip(n_hidden, kernel_size, strides)):\n",
        "            layer_list.append(\n",
        "                Conv2D(\n",
        "                    hidden,\n",
        "                    kernel_size=kernel,\n",
        "                    strides=stride,\n",
        "                    activation=activation,\n",
        "                    padding='SAME',\n",
        "                    name=\"dense_{}_1\".format(i),\n",
        "                ))\n",
        "        self.layer_list = layer_list\n",
        "        self.log_s_layer = Conv2D(\n",
        "            input_shape,\n",
        "            kernel_size=[3, 3],\n",
        "            strides=[1, 1],\n",
        "            padding='SAME',\n",
        "            kernel_initializer=\"zeros\",\n",
        "            activation=\"tanh\",\n",
        "            name=\"log_s\",\n",
        "        )\n",
        "        self.t_layer = Conv2D(\n",
        "            input_shape,\n",
        "            kernel_size=[3, 3],\n",
        "            strides=[1, 1],\n",
        "            padding='SAME',\n",
        "            kernel_initializer=\"zeros\",\n",
        "            name=\"t\",\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        y = x\n",
        "        for layer in self.layer_list:\n",
        "            y = layer(y)\n",
        "        log_s = self.log_s_layer(y)\n",
        "        t = self.t_layer(y)\n",
        "        return log_s, t\n",
        "\n",
        "class RealNVP(tfb.Bijector):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_shape,\n",
        "            forward_min_event_ndims=3,\n",
        "            validate_args: bool = False,\n",
        "            name=\"real_nvp\",\n",
        "            n_hidden=[512, 512],\n",
        "            **kargs,\n",
        "    ):\n",
        "        super(RealNVP, self).__init__(\n",
        "            validate_args=validate_args,\n",
        "            forward_min_event_ndims=forward_min_event_ndims,\n",
        "            name=name,\n",
        "        )\n",
        "\n",
        "        assert input_shape[-1] % 2 == 0\n",
        "        self.input_shape = input_shape\n",
        "        nn_layer = NN(\n",
        "            input_shape[-1] // 2,\n",
        "            n_hidden=n_hidden,\n",
        "        )\n",
        "        nn_input_shape = input_shape.copy()\n",
        "        nn_input_shape[-1] = input_shape[-1] // 2\n",
        "        x = tf.keras.Input(nn_input_shape)\n",
        "        log_s, t = nn_layer(x)\n",
        "        self.nn = Model(x, [log_s, t], name=self.name + \"/nn\")\n",
        "\n",
        "    def _forward(self, x):\n",
        "        x_a, x_b = tf.split(x, 2, axis=-1)\n",
        "        y_b = x_b\n",
        "        log_s, t = self.nn(x_b)\n",
        "        s = tf.exp(log_s)\n",
        "        y_a = s * x_a + t\n",
        "        y = tf.concat([y_a, y_b], axis=-1)\n",
        "        return y\n",
        "\n",
        "    def _inverse(self, y):\n",
        "        y_a, y_b = tf.split(y, 2, axis=-1)\n",
        "        x_b = y_b\n",
        "        log_s, t = self.nn(y_b)\n",
        "        s = tf.exp(log_s)\n",
        "        x_a = (y_a - t) / s\n",
        "        x = tf.concat([x_a, x_b], axis=-1)\n",
        "        return x\n",
        "\n",
        "    def _forward_log_det_jacobian(self, x):\n",
        "        _, x_b = tf.split(x, 2, axis=-1)\n",
        "        log_s, t = self.nn(x_b)\n",
        "        return tf.reduce_sum(log_s)\n",
        "    \n",
        "class Blockwise3D(tfb.Bijector):\n",
        "    def __init__(self,\n",
        "                 bijectors: list,\n",
        "                 block_sizes: list = None,\n",
        "                 validate_args=False,\n",
        "                 name=None):\n",
        "        if not name:\n",
        "            name = \"blockwise3D_of_\" + \"_and_\".join(\n",
        "                [b.name for b in bijectors])\n",
        "            name = name.replace(\"/\", \"\")\n",
        "        super(Blockwise3D, self).__init__(\n",
        "            forward_min_event_ndims=3,\n",
        "            validate_args=validate_args,\n",
        "            name=name,\n",
        "        )\n",
        "        self._bijectors = bijectors\n",
        "        self._block_sizes = block_sizes\n",
        "\n",
        "    @property\n",
        "    def bijectors(self):\n",
        "        return self._bijectors\n",
        "\n",
        "    @property\n",
        "    def block_sizes(self):\n",
        "        return self._block_sizes\n",
        "\n",
        "    def _forward(self, x):\n",
        "        split_x = (tf.split(x, len(self.bijectors), axis=-1)\n",
        "                   if self.block_sizes is None else tf.split(\n",
        "                       x, self.block_sizes, axis=-1))\n",
        "        split_y = [b.forward(x_) for b, x_ in zip(self.bijectors, split_x)]\n",
        "        y = tf.concat(split_y, axis=-1)\n",
        "        return y\n",
        "\n",
        "    def _inverse(self, y):\n",
        "        split_y = (tf.split(y, len(self.bijectors), axis=-1)\n",
        "                   if self.block_sizes is None else tf.split(\n",
        "                       y, self.block_sizes, axis=-1))\n",
        "        split_x = [b.inverse(y_) for b, y_ in zip(self.bijectors, split_y)]\n",
        "        x = tf.concat(split_x, axis=-1)\n",
        "        return x\n",
        "\n",
        "    def _forward_log_det_jacobian(self, x):\n",
        "        split_x = (tf.split(x, len(self.bijectors), axis=-1)\n",
        "                   if self.block_sizes is None else tf.split(\n",
        "                       x, self.block_sizes, axis=-1))\n",
        "        fldjs = [\n",
        "            b.forward_log_det_jacobian(x_, event_ndims=3)\n",
        "            for b, x_ in zip(self.bijectors, split_x)\n",
        "        ]\n",
        "        return sum(fldjs)\n",
        "\n",
        "    def _inverse_log_det_jacobian(self, y):\n",
        "        split_y = (tf.split(y, len(self.bijectors), axis=-1)\n",
        "                   if self.block_sizes is None else tf.split(\n",
        "                       y, self.block_sizes, axis=-1))\n",
        "        ildjs = [\n",
        "            b.inverse_log_det_jacobian(y_, event_ndims=3)\n",
        "            for b, y_ in zip(self.bijectors, split_y)\n",
        "        ]\n",
        "        return sum(ildjs)\n",
        "    \n",
        "class Squeeze3D(tfb.Bijector):\n",
        "    def __init__(\n",
        "            self,\n",
        "            factor=2,\n",
        "            forward_min_event_ndims=0,\n",
        "            inverse_min_event_ndims=0,\n",
        "            validate_args=False,\n",
        "            name=\"Squeeze\",\n",
        "    ):\n",
        "        self._factor = factor\n",
        "        super(Squeeze3D,\n",
        "              self).__init__(forward_min_event_ndims=forward_min_event_ndims,\n",
        "                             inverse_min_event_ndims=inverse_min_event_ndims,\n",
        "                             name=name,\n",
        "                             is_constant_jacobian=True)\n",
        "\n",
        "    @property\n",
        "    def factor(self):\n",
        "        return self._factor\n",
        "\n",
        "    def _forward(self, x):\n",
        "        (H, W, C) = x.shape[1:]\n",
        "        batch_size = tf.shape(x)[0:1]\n",
        "        tmp_shape = tf.concat(\n",
        "            [\n",
        "                batch_size,\n",
        "                (H // self.factor, self.factor, W // self.factor, self.factor,\n",
        "                 C),\n",
        "            ],\n",
        "            axis=0,\n",
        "        )\n",
        "        output_shape = tf.concat(\n",
        "            [\n",
        "                batch_size,\n",
        "                (H // self.factor, W // self.factor, C * self.factor**2)\n",
        "            ],\n",
        "            axis=0,\n",
        "        )\n",
        "        y = tf.reshape(x, tmp_shape)\n",
        "        y = tf.transpose(y, [0, 1, 3, 5, 2, 4])\n",
        "        y = tf.reshape(y, output_shape)\n",
        "        return y\n",
        "\n",
        "    def _inverse(self, y):\n",
        "        (H, W, C) = y.shape[1:]\n",
        "        batch_size = tf.shape(y)[0:1]\n",
        "        tmp_shape = tf.concat([\n",
        "            batch_size, (H, W, C // self.factor**2, self.factor, self.factor)\n",
        "        ],\n",
        "                              axis=0)\n",
        "        output_shape = tf.concat([\n",
        "            batch_size, (H * self.factor, W * self.factor, C // self.factor**2)\n",
        "        ],\n",
        "                                 axis=0)\n",
        "        x = tf.reshape(y, tmp_shape)\n",
        "        x = tf.transpose(x, [0, 1, 4, 2, 5, 3])\n",
        "        x = tf.reshape(x, output_shape)\n",
        "        return x\n",
        "\n",
        "    def _forward_log_det_jacobian(self, x):\n",
        "        return tf.constant(0.0, dtype=x.dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9Dxhm2026ns",
        "colab_type": "code",
        "outputId": "0ffa729d-9de0-4e24-cea2-6900d1e4f739",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def gen_flowSteps(\n",
        "    # for realnvp\n",
        "    input_shape: list,\n",
        "    n_hidden: list = [128, 128],\n",
        "    # for flowStep\n",
        "    k=4,\n",
        "    forward_min_event_ndims: int = 3,\n",
        "    validate_args: bool = False,\n",
        "    name: str = \"flow_step\",\n",
        "):\n",
        "    flow_step_list = []\n",
        "    for i in range(k):\n",
        "        t_lower_upper, t_permutation = trainable_lu_factorization(input_shape[-1])\n",
        "        flow_step_list.append(Actnorm(input_shape[-1]))\n",
        "        flow_step_list.append(\n",
        "            tfb.MatvecLU(\n",
        "                t_lower_upper, t_permutation, name=\"{}_{}/matveclu\".format(name, i)\n",
        "            )\n",
        "        ),\n",
        "        flow_step_list.append(\n",
        "            RealNVP(\n",
        "                input_shape=input_shape,\n",
        "                n_hidden=n_hidden,\n",
        "                validate_args=validate_args,\n",
        "                name=\"{}_{}/realnvp\".format(name, i),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    flowSteps = tfb.Chain(\n",
        "        list(reversed(flow_step_list)), validate_args=validate_args, name=name\n",
        "    )\n",
        "    return flowSteps\n",
        "\n",
        "\n",
        "def test_gen_flowSteps():\n",
        "    flowSteps = gen_flowSteps(\n",
        "        k=2, input_shape=[16, 16, 4], forward_min_event_ndims=0, name=\"flowstep_0\"\n",
        "    )\n",
        "    x = tf.keras.Input([16, 16, 4])\n",
        "    y = flowSteps(x)\n",
        "    # tf.keras.Model(x, y).summary()\n",
        "\n",
        "    x = tf.random.normal([6, 16, 16, 4])\n",
        "    y = flowSteps.forward(x)\n",
        "    z = flowSteps.inverse(y)\n",
        "    return tf.reduce_sum(z - x)\n",
        "\n",
        "\n",
        "test_gen_flowSteps()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoMN8ah426nu",
        "colab_type": "code",
        "outputId": "17aca6dd-fe30-4cc7-867a-68e9e24a9237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def gen_flow(input_shape, level=3, flow_step_args: dict = None):\n",
        "    def _gen_input_shapes(input_shape, level):\n",
        "        input_shape = input_shape\n",
        "        input_shapes = []\n",
        "        for i in range(level):\n",
        "            input_shape = [\n",
        "                input_shape[0] // 2,\n",
        "                input_shape[1] // 2,\n",
        "                input_shape[2] * 2,\n",
        "            ]\n",
        "            input_shapes.append(input_shape)\n",
        "        return input_shapes\n",
        "\n",
        "    input_shape[-1] = input_shape[-1] * 2\n",
        "    input_shapes = _gen_input_shapes(input_shape, level)\n",
        "\n",
        "    def _add_flow(_input_shapes, flow_step_args):\n",
        "        flow_lists = []\n",
        "        flow_lists.append(\n",
        "            Squeeze3D(name=\"Squeeze_{}\".format(level - len(_input_shapes)))\n",
        "        )\n",
        "        flowSteps = gen_flowSteps(\n",
        "           k=2,\n",
        "           input_shape=_input_shapes[0],\n",
        "           name=\"Flowsteps_{}\".format(level - len(_input_shapes)),\n",
        "        )\n",
        "        flow_lists.append(flowSteps)\n",
        "        if len(_input_shapes) != 1:\n",
        "            flow_lists.append(\n",
        "                Blockwise3D(\n",
        "                    [\n",
        "                        tfb.Identity(),\n",
        "                        tfb.Chain(\n",
        "                            list(reversed(_add_flow(_input_shapes[1:], flow_step_args)))\n",
        "                        ),\n",
        "                    ],\n",
        "                    name=\"Blockwise3D_{}\".format(level - len(_input_shapes)),\n",
        "                )\n",
        "            )\n",
        "        flow_lists.append(\n",
        "            tfb.Invert(\n",
        "                Squeeze3D(name=\"Unsqueeze_{}\".format(level - len(_input_shapes)))\n",
        "            )\n",
        "        )\n",
        "        return flow_lists\n",
        "\n",
        "    return tfb.Chain(list(reversed(_add_flow(input_shapes, level))))\n",
        "\n",
        "\n",
        "def test_gen_flow():\n",
        "    flow = gen_flow([32, 32, 1])\n",
        "    print(len(flow.trainable_variables))\n",
        "    x = tf.keras.Input([32, 32, 1])\n",
        "    y = flow.forward(x)\n",
        "    # tf.keras.Model(x, y).summary()\n",
        "    tf.keras.utils.plot_model(\n",
        "        tf.keras.Model(x, y), show_shapes=True, to_file=\"realnvp.png\"\n",
        "    )\n",
        "    x = tf.random.normal([3, 32, 32, 1])\n",
        "    y = flow.forward(x)\n",
        "    z = flow.inverse(y) \n",
        "    return tf.reduce_sum(z - x)\n",
        "\n",
        "test_gen_flow()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC1NIi5l26nw",
        "colab_type": "text"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGxPQH-h26nw",
        "colab_type": "text"
      },
      "source": [
        "### TransformDistribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNLHgd2126nx",
        "colab_type": "code",
        "outputId": "5499ac10-c87a-45cc-97af-6a526f240cda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "flow_bijector = gen_flow([32, 32, 1], level=3)\n",
        "print(len(flow_bijector.trainable_variables))\n",
        "flow = tfd.TransformedDistribution(\n",
        "    event_shape =[32, 32, 1],\n",
        "    distribution=tfd.Normal(loc=0.0, scale=1.0),\n",
        "    bijector=flow_bijector\n",
        ")\n",
        "print('trainable_variables: ', len(flow.bijector.trainable_variables))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66\n",
            "trainable_variables:  66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PClwsTD726nz",
        "colab_type": "text"
      },
      "source": [
        "### Loss, Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT-z2pd526nz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !rm -r checkpoints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3M6DZQY26n1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def loss():\n",
        "    return - tf.reduce_mean(flow.log_prob(targets['img'])) / (np.log(2.) * 32 * 32 * 1)\n",
        "\n",
        "\n",
        "optimizer = tf.optimizers.Adam(learning_rate=1e-5) \n",
        "# log = tf.summary.create_file_writer('checkpoints')\n",
        "avg_loss = tf.keras.metrics.Mean(name='loss', dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg_aWC8126n4",
        "colab_type": "code",
        "outputId": "03b87131-c7d0-4205-e5a0-04a44595ad18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "BATCH_SIZE=120\n",
        "train_dataset = train_dataset_raw.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "for target in train_dataset.take(1):\n",
        "    targets = target\n",
        "print(targets['img'].shape)\n",
        "with tf.GradientTape() as tape:\n",
        "    log_prob_loss = loss()\n",
        "grads = tape.gradient(log_prob_loss, flow.trainable_variables)\n",
        "print(log_prob_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(120, 32, 32, 1)\n",
            "tf.Tensor(898662300.0, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Gs5GQZe26n7",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vgvPFiN26n8",
        "colab_type": "code",
        "outputId": "3a868771-bc5e-455b-e175-cb5be5981e5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "n_epochs = 120\n",
        "from tqdm import tqdm\n",
        "flag = False\n",
        "for epoch in range(n_epochs):\n",
        "    if flag:\n",
        "        print('raise NaN')\n",
        "        break\n",
        "    for targets in tqdm(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            log_prob_loss = loss()\n",
        "        grads = tape.gradient(log_prob_loss, flow.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, flow.trainable_variables))\n",
        "        if tf.math.is_nan(log_prob_loss):\n",
        "            flag=True\n",
        "            break\n",
        "        avg_loss.update_state(log_prob_loss)\n",
        "        if tf.equal(optimizer.iterations % 1000, 0):\n",
        "            print(\"Step {} Loss {:.6f}\".format(optimizer.iterations, avg_loss.result()))\n",
        "        if tf.equal(optimizer.iterations % 100, 0):\n",
        "            # with log.as_default():\n",
        "            #     tf.summary.scalar(\"loss\", avg_loss.result(), step=optimizer.iterations)\n",
        "            #     avg_loss.reset_states()\n",
        "            avg_loss.reset_states()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.50it/s]\n",
            "500it [00:40, 13.35it/s]\n",
            "1it [00:00,  6.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=1000> Loss 92662.679688\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:39, 12.56it/s]\n",
            "500it [00:40, 12.44it/s]\n",
            "1it [00:00,  7.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=2000> Loss 7071.515625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.36it/s]\n",
            "500it [00:40, 12.34it/s]\n",
            "1it [00:00,  7.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=3000> Loss 2061.264404\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.23it/s]\n",
            "500it [00:40, 12.45it/s]\n",
            "1it [00:00,  7.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=4000> Loss 973.552979\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.46it/s]\n",
            "500it [00:40, 12.37it/s]\n",
            "1it [00:00,  7.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=5000> Loss 564.965332\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.40it/s]\n",
            "500it [00:40, 12.39it/s]\n",
            "1it [00:00,  7.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=6000> Loss 375.461456\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.41it/s]\n",
            "500it [00:40, 12.38it/s]\n",
            "1it [00:00,  7.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=7000> Loss 274.638672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.20it/s]\n",
            "500it [00:40, 12.31it/s]\n",
            "1it [00:00,  7.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=8000> Loss 215.854965\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.35it/s]\n",
            "500it [00:40, 12.40it/s]\n",
            "1it [00:00,  7.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=9000> Loss 186.470901\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.44it/s]\n",
            "500it [00:40, 12.39it/s]\n",
            "1it [00:00,  7.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=10000> Loss 158.986191\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.19it/s]\n",
            "500it [00:41, 12.05it/s]\n",
            "1it [00:00,  6.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=11000> Loss 105.353790\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.07it/s]\n",
            "500it [00:41, 12.06it/s]\n",
            "1it [00:00,  7.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=12000> Loss 50.882988\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.20it/s]\n",
            "500it [00:41, 11.96it/s]\n",
            "1it [00:00,  7.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=13000> Loss 25.865776\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.16it/s]\n",
            "500it [00:40, 12.21it/s]\n",
            "1it [00:00,  7.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=14000> Loss 2.423230\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.16it/s]\n",
            "500it [00:41, 11.91it/s]\n",
            "1it [00:00,  6.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=15000> Loss -23.790869\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.00it/s]\n",
            "500it [00:41, 12.03it/s]\n",
            "1it [00:00,  6.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=16000> Loss -51.666309\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.09it/s]\n",
            "500it [00:41, 12.17it/s]\n",
            "1it [00:00,  6.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=17000> Loss -83.094589\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:42, 11.84it/s]\n",
            "500it [00:41, 12.14it/s]\n",
            "1it [00:00,  6.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=18000> Loss -107.500755\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.00it/s]\n",
            "500it [00:42, 11.82it/s]\n",
            "1it [00:00,  7.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=19000> Loss -129.508560\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.20it/s]\n",
            "500it [00:41, 13.00it/s]\n",
            "1it [00:00,  7.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=20000> Loss -148.671402\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.25it/s]\n",
            "500it [00:40, 12.28it/s]\n",
            "1it [00:00,  7.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=21000> Loss -162.397186\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.22it/s]\n",
            "500it [00:40, 12.33it/s]\n",
            "1it [00:00,  7.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=22000> Loss -172.440140\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:44, 12.31it/s]\n",
            "500it [00:41, 12.06it/s]\n",
            "1it [00:00,  7.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=23000> Loss -214.407166\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.62it/s]\n",
            "500it [00:41, 12.08it/s]\n",
            "1it [00:00,  7.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=24000> Loss -246.476334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.09it/s]\n",
            "500it [00:41, 12.08it/s]\n",
            "1it [00:00,  6.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=25000> Loss -257.512848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.09it/s]\n",
            "500it [00:41, 12.04it/s]\n",
            "1it [00:00,  7.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=26000> Loss -263.458801\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.08it/s]\n",
            "500it [00:40, 12.24it/s]\n",
            "1it [00:00,  6.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=27000> Loss -267.299988\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 11.97it/s]\n",
            "500it [00:41, 12.11it/s]\n",
            "1it [00:00,  6.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=28000> Loss -270.574402\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.12it/s]\n",
            "500it [00:41, 12.09it/s]\n",
            "1it [00:00,  6.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=29000> Loss -272.203949\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 11.97it/s]\n",
            "500it [00:41, 12.06it/s]\n",
            "1it [00:00,  6.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=30000> Loss -273.450470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.12it/s]\n",
            "500it [00:41, 12.17it/s]\n",
            "1it [00:00,  7.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=31000> Loss -274.862793\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 13.01it/s]\n",
            "500it [00:40, 12.20it/s]\n",
            "1it [00:00,  7.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=32000> Loss -276.437683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.14it/s]\n",
            "500it [00:40, 12.24it/s]\n",
            "1it [00:00,  6.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=33000> Loss -278.137329\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.52it/s]\n",
            "500it [00:40, 12.25it/s]\n",
            "1it [00:00,  6.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=34000> Loss -280.028717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.15it/s]\n",
            "500it [00:40, 12.29it/s]\n",
            "1it [00:00,  7.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=35000> Loss -282.131836\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.21it/s]\n",
            "500it [00:40, 12.23it/s]\n",
            "1it [00:00,  7.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=36000> Loss -277.561768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.20it/s]\n",
            "500it [00:41, 12.17it/s]\n",
            "1it [00:00,  7.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=37000> Loss -282.231750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.16it/s]\n",
            "500it [00:40, 12.28it/s]\n",
            "1it [00:00,  7.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=38000> Loss -282.974884\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.46it/s]\n",
            "500it [00:40, 12.36it/s]\n",
            "1it [00:00,  7.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=39000> Loss -282.838409\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.23it/s]\n",
            "500it [00:40, 12.28it/s]\n",
            "1it [00:00,  6.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=40000> Loss -282.697449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.26it/s]\n",
            "500it [00:41, 12.31it/s]\n",
            "1it [00:00,  7.06it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=41000> Loss -282.894897\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 13.15it/s]\n",
            "500it [00:40, 12.29it/s]\n",
            "1it [00:00,  6.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=42000> Loss -283.240906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.25it/s]\n",
            "500it [00:40, 12.34it/s]\n",
            "1it [00:00,  7.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=43000> Loss -283.657440\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.27it/s]\n",
            "500it [00:40, 12.29it/s]\n",
            "1it [00:00,  6.93it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=44000> Loss -284.122223\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 13.18it/s]\n",
            "500it [00:41, 12.17it/s]\n",
            "1it [00:00,  7.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=45000> Loss -284.675812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.20it/s]\n",
            "500it [00:40, 12.23it/s]\n",
            "1it [00:00,  6.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=46000> Loss -285.305695\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.27it/s]\n",
            "500it [00:40, 12.23it/s]\n",
            "1it [00:00,  7.52it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=47000> Loss -286.019562\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.26it/s]\n",
            "500it [00:40, 12.29it/s]\n",
            "1it [00:00,  6.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=48000> Loss -286.851929\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.23it/s]\n",
            "500it [00:41, 12.11it/s]\n",
            "1it [00:00,  7.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=49000> Loss -287.786560\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.31it/s]\n",
            "500it [00:40, 12.22it/s]\n",
            "1it [00:00,  7.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=50000> Loss -288.878174\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.25it/s]\n",
            "500it [00:40, 12.26it/s]\n",
            "1it [00:00,  6.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=51000> Loss -289.765808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.18it/s]\n",
            "500it [00:41, 12.11it/s]\n",
            "1it [00:00,  7.52it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=52000> Loss -290.141724\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 11.98it/s]\n",
            "500it [00:40, 12.20it/s]\n",
            "1it [00:00,  7.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=53000> Loss -290.499084\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.15it/s]\n",
            "500it [00:42, 11.71it/s]\n",
            "1it [00:00,  6.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=54000> Loss -290.905457\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.14it/s]\n",
            "500it [00:41, 12.12it/s]\n",
            "1it [00:00,  7.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=55000> Loss -291.377167\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.73it/s]\n",
            "500it [00:41, 12.05it/s]\n",
            "1it [00:00,  6.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=56000> Loss -291.931152\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:41, 12.04it/s]\n",
            "500it [00:41, 12.03it/s]\n",
            "1it [00:00,  7.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=57000> Loss -292.566803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.25it/s]\n",
            "500it [00:41, 12.11it/s]\n",
            "1it [00:00,  6.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=58000> Loss -293.278809\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.22it/s]\n",
            "500it [00:41, 12.18it/s]\n",
            "1it [00:00,  7.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=59000> Loss -294.093201\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500it [00:40, 12.95it/s]\n",
            "500it [00:41, 12.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step <tf.Variable 'Adam/iter:0' shape=() dtype=int64, numpy=60000> Loss -212.380447\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3NAhJoM26n9",
        "colab_type": "text"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV2L45LA26n-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = test_dataset_raw.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGm9w9oI26n_",
        "colab_type": "code",
        "outputId": "41ddcba3-f766-4171-aaaf-747c05b0afc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "fig = plt.figure(figsize=(10, 2))\n",
        "for targets in test_dataset.take(1):\n",
        "    trg = targets[\"img\"]\n",
        "    inv = flow.bijector.forward(trg)\n",
        "    re_trg = flow.bijector.inverse(inv)\n",
        "\n",
        "print('log probability: ', (tf.reduce_mean(flow.log_prob(inv)) / (np.log(2.) * 32 * 32 * 1)).numpy())\n",
        "print(\"inv mean: \", tf.reduce_mean(inv).numpy(), \" std: \", tf.math.reduce_std(inv).numpy())\n",
        "print(\"re:trg mean: \", tf.reduce_mean(re_trg).numpy(), \" std: \", tf.math.reduce_std(re_trg).numpy())\n",
        "\n",
        "def  interporate(a, b, percent):\n",
        "  re_inv = np.array([(percent * a +  (1 - percent) * b)])\n",
        "  re_re_inv = flow.bijector.inverse(re_inv)\n",
        "  return tf.nn.relu6(re_re_inv - tf.reduce_mean(re_re_inv))\n",
        "# re_inv = np.array([(inv[0] + inv[1]) / 2.0])\n",
        "# print(re_inv.shape)\n",
        "#print(tf.reduce_mean(re_inv))\n",
        "# re_re_trg = flow.bijector.inverse(re_inv)\n",
        "\n",
        "\n",
        "ax = fig.add_subplot(1, 5, 1)\n",
        "ax.imshow(tf.squeeze(re_trg[0], axis=-1), aspect=\"auto\", cmap=\"gray_r\")\n",
        "\n",
        "ax = fig.add_subplot(1, 5, 2)\n",
        "ax.imshow(tf.squeeze(interporate(inv[0], inv[1], 0.75)[0], axis=-1), aspect=\"auto\", cmap=\"gray_r\")\n",
        "\n",
        "ax = fig.add_subplot(1, 5, 3)\n",
        "ax.imshow(tf.squeeze(interporate(inv[0], inv[1], 0.5)[0], axis=-1), aspect=\"auto\", cmap=\"gray_r\")\n",
        "\n",
        "ax = fig.add_subplot(1, 5, 4)\n",
        "ax.imshow(tf.squeeze(interporate(inv[0], inv[1], 0.25)[0], axis=-1), aspect=\"auto\", cmap=\"gray_r\")\n",
        "\n",
        "ax = fig.add_subplot(1, 5, 5)\n",
        "ax.imshow(tf.squeeze(re_trg[1], axis=-1), aspect=\"auto\", cmap=\"gray_r\")\n",
        "\n",
        "# ax = fig.add_subplot(2, 3, 3)\n",
        "# ax.imshow(tf.squeeze(re_inv[0], axis=-1), aspect=\"auto\", cmap=\"gray_r\")\n",
        "\n",
        "# ax = fig.add_subplot(2, 3, 6)\n",
        "#ax.imshow(tf.squeeze(tf.nn.relu6(re_re_trg[0] - tf.reduce_mean(re_re_trg)), axis=-1), aspect=\"auto\", cmap=\"gray_r\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log probability:  -1.3534642\n",
            "inv mean:  -0.4333917  std:  0.8253626\n",
            "re:trg mean:  -0.8132636  std:  0.5319577\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f524bd9a3c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAACOCAYAAAD+STYzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x144 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}